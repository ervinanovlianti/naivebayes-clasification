{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE2/lwgDgEoNjepziTcjrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ervinanovlianti/naivebayes-clasification/blob/main/dataminingexp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KKje1b3OESq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "columns = ['sent', 'class']\n",
        "rows = []\n",
        "rows = [['This is my book', 'stmt'],\n",
        "        ['They are novels', 'stmt'],\n",
        "        ['have you read this book','question'],\n",
        "        ['who is the author','question'],\n",
        "        ['what are the character','question'],\n",
        "        ['This is how I bought the book','stmt'],\n",
        "        ['I like fictions','stmt'],\n",
        "        ['what is your favourite book','question'],\n",
        "        ]\n",
        "training_data = pd.DataFrame(rows, columns=columns)\n",
        "training_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "stmt_docs = [row['sent'] for index, row in training_data.iterrows() if row['class'] == 'stmt']\n",
        "stmt_docs"
      ],
      "metadata": {
        "id": "lEgK-ZAGQKlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the price of the book\n",
        "\n",
        "vec_s = CountVectorizer()\n",
        "X_s = vec_s.fit_transform(stmt_docs)\n",
        "\n",
        "tdm_s = pd.DataFrame(X_s.toarray(), columns=vec_s.get_feature_names_out())\n",
        "\n",
        "tdm_s"
      ],
      "metadata": {
        "id": "gfm7I_h9RE08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_docs = [row['sent'] for index, row in training_data.iterrows() if row['class'] == 'question']\n",
        "q_docs\n",
        "\n",
        "vec_q = CountVectorizer()\n",
        "X_q = vec_q.fit_transform(q_docs)\n",
        "\n",
        "tdm_q = pd.DataFrame(X_q.toarray(), columns=vec_q.get_feature_names_out())\n",
        "tdm_q"
      ],
      "metadata": {
        "id": "IwG9tZh1R8cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list_s = vec_s.get_feature_names_out()\n",
        "count_list_s = X_s.toarray().sum(axis=0)\n",
        "freq_s = dict(zip(word_list_s, count_list_s))\n",
        "word_list_s\n",
        "freq_s"
      ],
      "metadata": {
        "id": "mKfppVIuTLOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_s=[]\n",
        "for word, count in zip(word_list_s, count_list_s):\n",
        "  prob_s.append(count/len(word_list_s))\n",
        "dict(zip(word_list_s, prob_s))\n"
      ],
      "metadata": {
        "id": "0888_0PcT2BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list_q = vec_q.get_feature_names_out()\n",
        "count_list_q = X_q.toarray().sum(axis=0)\n",
        "freq_q = dict(zip(word_list_q, count_list_q))\n",
        "word_list_q\n",
        "freq_q"
      ],
      "metadata": {
        "id": "IgdTKATcUPtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_q =[]\n",
        "for word, count in zip(word_list_q, count_list_q):\n",
        "  prob_q.append(count/len(word_list_q))\n",
        "dict(zip(word_list_q, prob_q))"
      ],
      "metadata": {
        "id": "bPVoY_F4UpBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs = [row['sent'] for index, row in training_data.iterrows()]\n",
        "\n",
        "vec = CountVectorizer()\n",
        "X = vec.fit_transform(docs)\n",
        "\n",
        "total_features = len(vec.get_feature_names_out())\n",
        "total_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxDr3ZMFVC7z",
        "outputId": "b57a5d52-83c4-4482-9f0f-4080ece62da3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_cnts_features_s = count_list_s.sum(axis=0)\n",
        "total_cnts_features_q = count_list_q.sum(axis=0)\n",
        "\n",
        "print(total_cnts_features_s)\n",
        "print(total_cnts_features_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKXnL2v-WGDl",
        "outputId": "759c348f-c39c-4c36-b0ed-f6fa5f5d7ca7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK -> Natural Language Processing\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYt85omOWe_T",
        "outputId": "133ae1b8-a9fa-4db2-f4e1-087f44476649"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "new_sentence = 'my favorite book is pulang by tere liye'\n",
        "new_word_list = word_tokenize(new_sentence)\n",
        "print(new_word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsNvxb7UWyO5",
        "outputId": "a1592886-765d-435c-d2e2-8469569cf8e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'favorite', 'book', 'is', 'pulang', 'by', 'tere', 'liye']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_s_with_ls = []\n",
        "for word in new_word_list:\n",
        "  if word in freq_s.keys():\n",
        "    count = freq_s[word]\n",
        "  else:\n",
        "    count = 0\n",
        "  prob_s_with_ls.append((count+1)/(total_cnts_features_s + total_features))\n",
        "dict(zip(new_word_list, prob_s_with_ls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlKbELrUXN2K",
        "outputId": "35cb0e15-762e-4bbd-96d6-9c2efe213d0e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'my': 0.05555555555555555,\n",
              " 'favorite': 0.027777777777777776,\n",
              " 'book': 0.08333333333333333,\n",
              " 'is': 0.08333333333333333,\n",
              " 'pulang': 0.027777777777777776,\n",
              " 'by': 0.027777777777777776,\n",
              " 'tere': 0.027777777777777776,\n",
              " 'liye': 0.027777777777777776}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_q_with_ls = []\n",
        "for word in new_word_list:\n",
        "  if word in freq_q.keys():\n",
        "    count = freq_q[word]\n",
        "  else:\n",
        "    count = 0\n",
        "  prob_q_with_ls.append((count+1)/(total_cnts_features_q + total_features))\n",
        "dict(zip(new_word_list, prob_q_with_ls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrucAGlKYDnM",
        "outputId": "9e420616-08ee-4df5-8d30-44271165ca36"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'my': 0.02564102564102564,\n",
              " 'favorite': 0.02564102564102564,\n",
              " 'book': 0.07692307692307693,\n",
              " 'is': 0.07692307692307693,\n",
              " 'pulang': 0.02564102564102564,\n",
              " 'by': 0.02564102564102564,\n",
              " 'tere': 0.02564102564102564,\n",
              " 'liye': 0.02564102564102564}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "prob_s_ = 1\n",
        "for i in range(len(prob_s_with_ls)):\n",
        "  prob_s_ = prob_s_ * prob_s_with_ls[i]\n",
        "print(prob_s_)\n",
        "nilai_akhir_s = prob_s_ * 0.5\n",
        "print('Nilai akhir = ', nilai_akhir_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG8daCDZYZpy",
        "outputId": "4f74e7a3-a930-43ce-eeca-301f0d8e02d0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.380467472191432e-12\n",
            "Nilai akhir =  3.190233736095716e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "prob_q_ = 1\n",
        "for i in range(len(prob_q_with_ls)):\n",
        "  prob_q_ = prob_q_ * prob_q_with_ls[i]\n",
        "print(prob_q_)\n",
        "nilai_akhir_q = prob_q_ * 0.5\n",
        "print('Nilai akhir = ', nilai_akhir_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22wZacSDZAZC",
        "outputId": "8934100e-154b-44b1-9ef2-fb20d55e9689"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6816114401100912e-12\n",
            "Nilai akhir =  8.408057200550456e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Kesimpulan\n",
        "if nilai_akhir_s > nilai_akhir_q:\n",
        "  print('Kalimat diatas termasuk kalimat statement')\n",
        "else:\n",
        "  print('Kalimat diatas termasuk kalimat question')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwIgPvgKaxBd",
        "outputId": "5be0c719-ccac-49f5-81c6-e16b336697a2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat diatas termasuk kalimat statement\n"
          ]
        }
      ]
    }
  ]
}